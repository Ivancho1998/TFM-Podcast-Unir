{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t_vQhYzT2Kw",
        "outputId": "8a37ef0c-4008-45e6-c809-cf7015704867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-ttf4h64k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-ttf4h64k\n",
            "  Resolved https://github.com/openai/whisper.git to commit 90db0de1896c23cbfaf0c58bc2d30665f709f170\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper==20240930)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803583 sha256=fe0b7e53015c4a80e078076ca8cd77b49d11db36205f9063f41aaf062ee3a56a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a25qju70/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,196 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,469 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,481 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,364 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,516 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,755 kB]\n",
            "Fetched 16.4 MB in 4s (3,957 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.8.30)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy textblob vaderSentiment scikit-learn pandas\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!apt update && apt install -y ffmpeg\n",
        "!pip install spacy sklearn\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install deep-translator --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show deep-translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w8tzZarbpRi",
        "outputId": "5d57924d-b15f-4432-c303-5d32008f57a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: deep-translator\n",
            "Version: 1.11.4\n",
            "Summary: A flexible free and unlimited python tool to translate between different languages in a simple way using multiple translators\n",
            "Home-page: https://github.com/nidhaloff/deep_translator\n",
            "Author: Nidhal Baccouri\n",
            "Author-email: nidhalbacc@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: beautifulsoup4, requests\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from deep_translator import GoogleTranslator, PonsTranslator, LingueeTranslator\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Y5vyLmhNT4Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Transcripción con Whisper\n",
        "def transcribe_audio(file_path):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(file_path, language=\"es\")\n",
        "    return result[\"text\"]\n",
        "\n",
        "# Paso 2: Preprocesamiento con spaCy\n",
        "def preprocess_text(text):\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Paso 3: Análisis de sentimientos con TextBlob\n",
        "def analyze_sentiment_textblob(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity  # Rango de -1 a 1\n",
        "    return \"positivo\" if sentiment > 0 else \"negativo\" if sentiment < 0 else \"neutral\"\n",
        "\n",
        "# Paso 4: Análisis de sentimientos con VADER\n",
        "def analyze_sentiment_vader(text):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return \"positivo\" if scores[\"compound\"] > 0 else \"negativo\" if scores[\"compound\"] < 0 else \"neutral\"\n",
        "\n",
        "# Paso 5: Análisis de temas con LDA\n",
        "def analyze_topics(text, num_topics=3):\n",
        "    vectorizer = CountVectorizer(stop_words=\"spanish\")\n",
        "    doc_term_matrix = vectorizer.fit_transform([text])\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(doc_term_matrix)\n",
        "\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for idx, topic in enumerate(lda.components_):\n",
        "        top_terms = [terms[i] for i in topic.argsort()[-5:]]\n",
        "        topics.append(f\"Tema {idx+1}: {', '.join(top_terms)}\")\n",
        "    return topics\n",
        "\n",
        "# Paso 6: Resumen de ideas\n",
        "def summarize_text(text, num_sentences=3):\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    sentence_scores = Counter()\n",
        "\n",
        "    for sent in sentences:\n",
        "        for word in preprocess_text(sent).split():\n",
        "            sentence_scores[sent] += 1\n",
        "\n",
        "    summary = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "    return \"\\n\".join(summary)\n",
        "\n",
        "# Paso 7: Entrenar modelo SVM para análisis de sentimientos\n",
        "def train_svm_model():\n",
        "    data = pd.DataFrame({\n",
        "        \"text\": [\"Me encanta este producto\", \"Odio este servicio\", \"Es simplemente aceptable\", \"Muy bueno\", \"Terrible\"],\n",
        "        \"sentiment\": [\"positivo\", \"negativo\", \"neutral\", \"positivo\", \"negativo\"]\n",
        "    })\n",
        "    data[\"processed_text\"] = data[\"text\"].apply(preprocess_text)\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(data[\"processed_text\"])\n",
        "    y = data[\"sentiment\"]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = SVC(kernel=\"linear\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    print(classification_report(y_test, predictions))\n",
        "\n",
        "    return model, vectorizer\n",
        "\n",
        "# Cargar el modelo de spaCy para español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Lista personalizada de stop words en español\n",
        "spanish_stop_words = nlp.Defaults.stop_words\n",
        "\n",
        "# Función para preprocesar texto y eliminar stop words en español\n",
        "def preprocess_text_spanish(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if token.lemma_ not in spanish_stop_words and not token.is_punct]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Análisis de temas en español\n",
        "def analyze_topics_spanish(text, num_topics=3):\n",
        "    vectorizer = CountVectorizer(stop_words=spanish_stop_words)\n",
        "    doc_term_matrix = vectorizer.fit_transform([text])\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(doc_term_matrix)\n",
        "\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for idx, topic in enumerate(lda.components_):\n",
        "        top_terms = [terms[i] for i in topic.argsort()[-5:]]\n",
        "        topics.append(f\"Tema {idx+1}: {', '.join(top_terms)}\")\n",
        "    return topics\n",
        "\n",
        "# Traducción y análisis de temas en inglés\n",
        "def translate_and_analyze_topics(text, num_topics=3):\n",
        "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "    doc_term_matrix = vectorizer.fit_transform([text])\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
        "    lda.fit(doc_term_matrix)\n",
        "\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for idx, topic in enumerate(lda.components_):\n",
        "        top_terms = [terms[i] for i in topic.argsort()[-5:]]\n",
        "        topics.append(f\"Tema {idx+1}: {', '.join(top_terms)}\")\n",
        "    return topics, translations"
      ],
      "metadata": {
        "id": "0c__GFdpUP8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVeesRAtUoDn",
        "outputId": "fabdbe94-1d60-4da5-f118-b702ca0bdcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path = '/content/drive/MyDrive/YouTube Audios/CHINOLO44_1_ffmpeg.mp3'"
      ],
      "metadata": {
        "id": "WGqhp9ICUuzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcripción\n",
        "text = transcribe_audio(audio_file_path)\n",
        "print(f\"Transcripción:\\n{text}\")\n",
        "\n",
        "# Preprocesamiento\n",
        "processed_text = preprocess_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ql93UV0UYr-",
        "outputId": "78064943-9a99-4c58-ea6a-b5acaefb8ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 183MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcripción:\n",
            " que pasa chavales bienvenidos a chinolo bueno noticia triste no noticia triste me acabo de enterar y digo joder yo tengo que hacer un vídeo de Antonio no todo improvisación no tengo nada no tengo nada notado porque me acabo de enterar como ya os digo y la verdad me ha puesto hasta un poco triste no la noticia porque para los que llevamos siguiendo en los muchos años etcétera Antonio siempre ha sido un tío que estaba ahí en el sistema digamos no hasta o en la hasta o en la pomada y me ha puesto hasta triste no vamos a dedicar unas palabras ha improvisado un poco he pensado de cosas buenas y cosas malas no porque ya os digo que me ha puesto hasta un poco triste se van los mejores no entonces bueno lo primero que hay que yo el titular no el titular lo diría de ganar wols a una prejuvilación anticipada no o sea hace de un mes estaba diciendo que iba a ganar la lequí y va a ganar wols bueno al final la realidad es la que es no se puede luchar contra esos chavales supongo que sea un vídeo contó esto pues da pena no chavales da pena eran tiempos buenos no éramos más jóvenes todos hace siete ocho años no me acuerdo ya a diez seis no lo sé que siempre te salía el tío este el peluquilla es no por internet y te hacía gracia y bueno parece que le faltaba una cocida que era un chavala si un poco tontico tal cayó en gracia y hoy pues cayó en gracia y la comunidad los quiso al final la comunidad la comunidad acabó queriéndolo no eso falta hoy mucho y en día también ese amor genuino en la comunidad de que salgan personajes como este y no personajes metidos con calzador como el yo y ahí está gente sino gente como Antonio y nocentes era un chavarín no un chavarín ya os digo de metro no sé cuando mi de metro sesenta que le faltaba una cocida que era muy mágico y cayó en gracia no luego está en hombre pues como olvidar o sea como olvidar aneth dotas aneth dotas de antonio muchos y cuando la sabéis pero una de las aneth dotas más famosa de antonio es que se limpió la cara con la toalla con la que yizú que se limpia va al culo o sea este tipo de aneth dotas solo no las puede regar a alguien como antonio no el mismo lo dijo o sea se estaba lavando la cara y cogió una toalla para para limpiarse el agua de la cara y dijo hostia hostia y era la toalla con la que yizú que se limpia va al culo tremendo no les tuvo liendo la cara a mierda medio día no esto no lo digo de malas son aneth dotas uyas que coño calan en la comunidad no hizo gracia entonces no se me dan pojo un poco de nostalgia chavarés porque pues eso era un buen chaval era la época dorada del lol no hace años dos mil diecisiete dos mil dieciocho mil diecinueve no acabó para mí la época dorada donde teníamos este tipo de personajes mas inocentes no había tanta maldad tantas ganas de vender te basura de vender teumo etc era era una época mejor no chavades era una época mejor y desde aquí le mando un saludo a antonio aunque luego le va a meter algún palo pero pero la verdad que me ha dado pena no me ha dado pena luego claro antonio también hizo un legado no hizo el legado de yayans de guina max hizo un legado el empezo ahí y acabó ahí su carrera fue querido no fue querido supongo que sí hasta qué punto lo uso yayans pod ser la imagen del clún no porque a lo mejor el mismo antonio generaba más generaba más atracción hacia los clientes y los big wells que el propio yayans no entonces no sé hasta qué punto se ha aprovechado de esos yayans para tenerlo digamos era como la igual que fake es para ti guan la imagen para yayans era de antonio o sea eso está clarísimo y hizo un legado ahí y bueno hay ha pasado pues más que nada sin pena ni gloria no él ha conseguido hacerse su extremital y yo creo que vivirá de eso no ahora será creador de contenido y vivirá mejor no pero vamos luego le pegaremos algún palo porque ahora estoy hablando de él como persona y como persona hombre pues se le tiene cierto cariño no al antonio luego también ha sido como un compañero más no aunque no lo conozcamos no porque ha ido lo hemos visto evolucionar de niño a hombre no o sea fijaros esta foto y fijado las fotos actuales o sea parece un tío que se ha metido en la roba parece un tío que parece otra persona no parece otra persona o sea de un chavalino cente tontico tal ahora está claro que aún le sigue faltando alguna cocida pero tiene una imagen así más de maduro más agresiva más mayor no ha crecido y esa evolución la hemos visto los que seguimos el héroe no y por eso os digo que me da un poco de pena no que se retire pero bueno actualmente de la imagen que tienen pues es un poco más no sé tampoco lo veo muy poco más de matón no hay con sus tatuajes de poca y etcétera no sé si llevará lo de amor de madre tatuado en algún lado pero supongo que sí pero bueno es una lastiva no sobre todo por el tema de que se acaba la edad dorada de los no chavales estamos viendo la caída del lol estamos viendo la decadencia del lol y va a ir gente como antonio se va todo el mundo se ira yendo y quedan los cuatro datos y esto volver a ser tierra de nadie no y es una pena no entonces como persona ya os digo cantoño pues se le tiene cariño no y es un tío así de este palo que era un poco inocente tal y que yo en gracia y bueno se le tiene cariño ahora si hablamos de el como jugador del lol para mí igual que he dicho cosas buenas para mí como jugador del hoy representa toda la mediocridad que hay hoy en día en la lec en el ol europeo representa la mediocridad total ya no lo dijo a tila no que él nunca había trabajado serio para ser nada en el y como jugador para mí representa eso no la falta de ambición la mediocridad el conformismo de decir hoy yo soy aquí la imagen de ya y en su día número 1 me conformo no una persona pues que no trabaja duro por sus sueños ok no trabaja duro prioriza otras cosas que ojo todos tenemos de hecho hacer eso igual yo soy furbolista tengo mucho talento y no me apetece entrenar duro y con el talento me va a lepa jugar yo que se en el albacete pero podría ver jugar en el barcelona subirá tenido talento no yo había trabajado yo creo cantoño lo podían haber explotado mejor si lo hubieran rodeado de un buen staf un buen coach etcétera Antonio podía haber llegado algo no pero es la viva imagen o sea es que ya me perdonaréis que diga esto de antoño el día que se retira ya os he dicho cosas buenas pero como jugador para mí es la viva imagen de la mediocridad del conformismo lo que digo de no luchar por tus sueños o sea representa todo lo que es el sector o sea no tiene más representa absolutamente todos los parámetros no todas las características de la gente que hay hoy en día en el loll donde para ellos el loll es bueno un sitio donde divertirte de vez en cuando pero ni mucho menos un trabajo ni mucho menos una profesión sería no a la que dediques horas etcétera jugando solo que un poco más no entonces muy medio crey luego también cumplió el sueño de jugar en la leg que ha sido como la agenda del pastel de su carrera que ojo está claro que no tenía nivel para jugar sobre todo en línea o sea que diga lo contrario me puedo comer los cojones no tenía nivel para jugar en línea siempre supergapéado en plan 60 minios por detrás a minuto 15 o sea unas gapéadas del copón no es verdad que luego en tinfights pues lo hacía medio bien o sea para tinfights y que varía pegado le faltaba mucho trabajo en línea le faltaba pues eso meterle horas tomate lo en serio estudiar el juego las interacciones pues lo que falta hoy en la leg o sea la mediocridad absoluta entonces cumplió ese sueño de jugar en la leg que yo no sé si se lo concedieron porque era un proyecto del de yayas que no que era una fumada de mierda y dije no pues me en gato yo pegate aquí un un expliqui quieres y antes de jubilarte no si se sabía ya que se iba jubilando a los esos son especulaciones entonces como jugador mal no chaval es mal como profesional mal sinceramente siempre la preocupa más la gachiesa con la que estuvo dos años haciendo el canelo comense una paella y comense un chuleto pues siempre la gusta más eso que lo es entonces es duro es duro entonces nada chaval tampoco quiero decir mucho más la verdad que ya os digo que me pone triste no porque se antonio por tal sino porque por ver que se acaba una época no se acaba una época en el gol una época dorada y ya os digo que como persona pues antonio me parece muy bien no tengo nada en su contra en absoluto es más creo que hasta me charía unas buenas risas con él si lo conociera pero pues como jugador del oliprofesional le tengo que meter el palo porque es que representa la más absoluta mediocridad y la falta de ganas y desacrificio no entonces nada chavales ordejo en esta noche del luto vamos a hacer una semana del luto en el canal y nada un saludo al peluquillas desde aquí peluquillas un saludo de comandante y nos vemos en el siguiente vídeo chávales un abrazo a todos y un abrazo antonio suscribiros y joputas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Análisis de sentimientos\n",
        "sentiment_textblob = analyze_sentiment_textblob(processed_text)\n",
        "sentiment_vader = analyze_sentiment_vader(processed_text)\n",
        "\n",
        "# Análisis de temas\n",
        "topics = analyze_topics(processed_text)\n",
        "\n",
        "# Resumen de ideas\n",
        "summary = summarize_text(text)\n",
        "\n",
        "# Entrenamiento del modelo SVM\n",
        "svm_model, vectorizer = train_svm_model()\n",
        "sentiment_svm = svm_model.predict(vectorizer.transform([processed_text]))[0]\n",
        "\n",
        "# Resultados\n",
        "print(\"\\nResultados del análisis de sentimientos:\")\n",
        "print(f\"TextBlob: {sentiment_textblob}\")\n",
        "print(f\"VADER: {sentiment_vader}\")\n",
        "print(f\"SVM: {sentiment_svm}\")\n",
        "\n",
        "print(\"\\nTemas principales:\")\n",
        "for topic in topics:\n",
        "  print(topic)\n",
        "\n",
        "print(\"\\nResumen del texto:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "eDZ4FUMkVWW-",
        "outputId": "8515f179-941a-4bf9-837b-7fc8c9cc637b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidParameterError",
          "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'spanish' instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0039272869c5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Análisis de temas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Resumen de ideas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bce6f11d8f4f>\u001b[0m in \u001b[0;36manalyze_topics\u001b[0;34m(text, num_topics)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spanish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdoc_term_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpartial_fit_and_fitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m                 \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             with config_context(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0maccepted\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m--> 666\u001b[0;31m         validate_parameter_constraints(\n\u001b[0m\u001b[1;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             raise InvalidParameterError(\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0;34mf\"The {param_name!r} parameter of {caller_name} must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;34mf\" {constraints_str}. Got {param_val!r} instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'spanish' instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir texto en fragmentos\n",
        "def split_text(text, chunk_size=100):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# Traducir fragmentos y unir\n",
        "def translate_text_in_chunks(text, translator=\"Google\"):\n",
        "    chunks = split_text(text, chunk_size=50)\n",
        "    translated_chunks = []\n",
        "\n",
        "    if translator == \"Google\":\n",
        "        for chunk in chunks:\n",
        "            translated_chunks.append(GoogleTranslator(source=\"es\", target=\"en\").translate(chunk))\n",
        "    elif translator == \"Pons\":\n",
        "        for chunk in chunks:\n",
        "            translated_chunks.append(PonsTranslator(source=\"es\", target=\"en\").translate(chunk))\n",
        "    elif translator == \"Linguee\":\n",
        "        for chunk in chunks:\n",
        "            translated_chunks.append(LingueeTranslator(source=\"es\", target=\"en\").translate(chunk))\n",
        "    else:\n",
        "        raise ValueError(\"Translator not supported\")\n",
        "\n",
        "    return ' '.join(translated_chunks)\n",
        "\n",
        "# Uso\n",
        "translated_text_google = translate_text_in_chunks(text, translator=\"Google\")\n",
        "translated_text_pons = translate_text_in_chunks(text, translator=\"Pons\")\n",
        "translated_text_linguee = translate_text_in_chunks(text, translator=\"Linguee\")\n",
        "\n",
        "# Comparar traducciones\n",
        "print(\"\\nGoogle Translator:\\n\", translated_text_google)\n",
        "print(\"\\nPons Translator:\\n\", translated_text_pons)\n",
        "print(\"\\nLinguee Translator:\\n\", translated_text_linguee)\n"
      ],
      "metadata": {
        "id": "wQWm52z8ev2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opción 2: Traducción y análisis en inglés\n",
        "print(\"\\nAnálisis de temas en inglés:\")\n",
        "topics_english, translations = translate_and_analyze_topics(audio_file_path)\n",
        "for topic in topics_english:\n",
        "    print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "VaC0xgf4clnu",
        "outputId": "35a3ffaa-6032-45cd-ead0-073ee36355ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Análisis de temas en inglés:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotValidLength",
          "evalue": "/content/drive/MyDrive/YouTube Audios/CHINOLO44_1_ffmpeg.mp3 --> Text length need to be between 0 and 50 characters",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotValidLength\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-80175a7869a2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Opción 2: Traducción y análisis en inglés\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAnálisis de temas en inglés:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtopics_english\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_and_analyze_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics_english\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bce6f11d8f4f>\u001b[0m in \u001b[0;36mtranslate_and_analyze_topics\u001b[0;34m(text, num_topics)\u001b[0m\n\u001b[1;32m    104\u001b[0m     translations = {\n\u001b[1;32m    105\u001b[0m         \u001b[0;34m\"GoogleTranslator\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGoogleTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"es\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;34m\"PonsTranslator\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPonsTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"es\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;34m\"LingueeTranslator\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLingueeTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"es\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deep_translator/pons.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, word, return_all, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtranslated\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_input_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_same_source_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deep_translator/validate.py\u001b[0m in \u001b[0;36mis_input_valid\u001b[0;34m(text, min_chars, max_chars)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotValidPayload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_chars\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mmin_chars\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotValidLength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotValidLength\u001b[0m: /content/drive/MyDrive/YouTube Audios/CHINOLO44_1_ffmpeg.mp3 --> Text length need to be between 0 and 50 characters"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Opción 1: Análisis en español\n",
        "print(\"Análisis de temas en español:\")\n",
        "processed_text = preprocess_text_spanish(audio_file_path)\n",
        "topics_spanish = analyze_topics_spanish(processed_text)\n",
        "for topic in topics_spanish:\n",
        "    print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf0a1Vwxd0_b",
        "outputId": "414264b0-e3b4-4950-d741-edf0c39923a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Análisis de temas en español:\n",
            "Tema 1: content, drive, mp3, mydrive, youtube\n",
            "Tema 2: content, drive, mp3, mydrive, youtube\n",
            "Tema 3: content, drive, mp3, mydrive, youtube\n",
            "Tema 4: content, drive, mp3, mydrive, youtube\n",
            "Tema 5: content, drive, mp3, mydrive, youtube\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir texto en fragmentos\n",
        "def split_text(text, chunk_size=100):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# Traducir fragmentos y unir\n",
        "def translate_text_in_chunks(text, translator):\n",
        "    chunks = split_text(text, chunk_size=200)\n",
        "    translated_chunks = []\n",
        "\n",
        "    if translator == \"Google\":\n",
        "        for chunk in chunks:\n",
        "            translated_chunks.append(GoogleTranslator(source=\"es\", target=\"en\").translate(chunk))\n",
        "    elif translator == \"Pons\":\n",
        "        for chunk in chunks:\n",
        "            translated_chunks.append(PonsTranslator(source=\"es\", target=\"en\").translate(chunk))\n",
        "    elif translator == \"Linguee\":\n",
        "        for chunk in chunks:\n",
        "            translated_chunks.append(LingueeTranslator(source=\"es\", target=\"en\").translate(chunk))\n",
        "    else:\n",
        "        raise ValueError(\"Translator not supported\")\n",
        "\n",
        "    return ' '.join(translated_chunks)\n",
        "\n",
        "texto = split_text(processed_text)\n",
        "texto"
      ],
      "metadata": {
        "id": "V5_51xUheGhL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90370b81-3d3c-4cd7-f742-dd4e996f7b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pasar chaval bienvenido chinolo noticia triste noticia triste acabar enterar decir joder vídeo Antonio improvisación notado acabar enterar decir poner triste noticia llevar seguir año etcétera Antonio tío sistema decir pomada poner triste dedicar palabra improvisar pensar cosa cosa mala decir poner triste mejor titular titular decir ganar wols prejuvilación anticipado mes decir ir ganar lequí ganar wols realidad luchar chaval suponer vídeo contar pena chaval pena tiempo ser joven año salir tío peluquilla internet hacer gracia faltar cocida chavalo tontico caer gracia caer gracia comunidad querer comunidad comunidad acabar queriéndolo faltar amor genuino comunidad salir personaje personaje metido calzador',\n",
              " 'gente gente Antonio nocent chavarín chavarín decir metro metro sesenta faltar cocida mágico caer gracia hombre olvidar olvidar aneth dota aneth dota antonio sabéis aneth dota famoso antonio limpiar cara toalla yizú limpiar culo tipo aneth dota regar alguien antonio lavar cara coger toalla limpiar él agua cara hostia hostia toalla yizú limpiar culo tremendo leer cara mierda decir mala aneth dota uya coño calar comunidad gracia pojo nostalgia chavarés chaval época dorado lol año mil diecisiete mil dieciocho mil diecinueve acabar época dorado tener tipo personaje inocente tanto maldad tanto gana vender basura vender teumo etc época chavad época',\n",
              " 'mandar saludo antonio meter palo pena pena antonio legado legado yayan guina max legado empezo acabar carrera querer querer suponer punto yayans pod imagen clún antonio generar generar atracción cliente big wells yayan punto aprovechar yayan tener él decir fakir guan imagen yayan antonio clarísimo legado pena gloria conseguir hacer él extremital vivir creador contenido vivir pegarar palo hablar persona persona hombre cariño antonio compañero conocer ir ver evolucionar niño hombre fijaro foto fijar foto actual tío meter roba tío persona persona chavalino cente tontico faltar cocida imagen maduro agresivo crecer evolución ver seguir héroe decir pena retirar actualmente imagen',\n",
              " 'ver matón tatuaj etcétera llevar amor madre tatuado suponer lastiva tema acabar edad dorado chaval ver caída lol ver decadencia lol gente antonio mundo irar ir quedar dato volver tierra pena persona decir cantoño cariño tío palo inocente gracia cariño hablar jugador lol cosa jugador representar mediocridad lec ol europeo representar mediocridad tila trabajar serio jugador representar falta ambición mediocridad conformismo imagen número 1 conformar persona trabajar duro sueño ok trabajar duro priorizar cosa ogir furbolista talento apetecer entrenar duro talento lepa jugar albacete jugar barcelona subir talento trabajar cantoño poder explotar haber rodear staf coach etcétera Antonio poder llegar',\n",
              " 'vivo imagen perdonaréis decir antoño retirar cosa jugador vivo imagen mediocridad conformismo decir luchar sueño representar sector representar absolutamente parámetro característica gente loll loll sitio divertirte trabajo profesión dediqu hora etcétera jugar crey cumplir sueño jugar leg agenda pastel carrera ojo nivel jugar línea decir contrario comer cojón nivel jugar línea supergapéado plan 60 minio minuto 15 gapéada copón tinfights hacer tinfights variar pegado faltar trabajo línea faltar meter él hora tomatir serio estudiar juego interacción faltar leg mediocridad absoluto cumplir sueño jugar leg conceder proyecto yayas fumada mierda decir gato pegatir expliqui quier jubilarte saber ir jubilar especulación jugador',\n",
              " 'chaval profesional sinceramente preocupa gachiesa año canelo comense paella comense chuleto gusta duro duro chaval querer decir poner triste antonio acabar época acabar época gol época dorado decir persona antonio absoluto chacer risa conocer jugador oliprofesional meter palo representar absoluto mediocridad falta gana desacrificio chaval ordejo noche luto semana luto canal saludo peluquilla peluquil él saludo comandante ver vídeo chával abrazo abrazo antonio suscribiro joputa']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_and_analyze_topics"
      ],
      "metadata": {
        "id": "6dFH-VE-7Kc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uso\n",
        "translated_text_google = translate_text_in_chunks(text, translator=\"Google\")\n",
        "\n",
        "\n",
        "# Comparar traducciones\n",
        "print(\"\\nGoogle Translator:\\n\", translated_text_google)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-PFag234u0a",
        "outputId": "494da26c-fd42-4020-825d-0b563d53a901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Google Translator:\n",
            " What's up guys welcome to Chinolo well sad news no sad news I just found out and I say fuck I have to make a video of Antonio not all improvisation I have nothing I have nothing noticed because I just found out as I already told you and the truth is I It has even made me a little sad, the news because for those of us who have been following Antonio for many years etcetera, he has always been a guy who was there in the system, let's say, not until or in the middle of it, and it has even made me sad, no. We are going to dedicate a few words, I have improvised a little, I have thought of good things and bad things, no, because as I told you, it has even made me a little sad, the best are leaving, no, well, the first thing is that I would say the headline, no, the headline of winning wols to an early retirement no, that is, a month ago I was saying that I was going to win the lequí and wols is going to win, well in the end the reality is what it is, you can't fight against those kids, I suppose it's a video that told this Well, it's a shame, guys, it's a shame, those were good times, we weren't all younger, seven or eight years ago, I don't remember, at ten or six, I don't know, you always came across this guy with the hairdo on the internet and you thought it was funny and well, it seems that He was missing a stew that was a girl, if a little silly, he fell into grace and today, well, he fell into grace and the community loved him in the end, the community, the community ended up loving him, isn't that missing a lot today and today also that genuine love in the community that characters like this come out and not characters shoehorned in like me and there are people but people like Antonio and Nocentes was a kid not a kid as I tell you, I don't know when my meter sixty that was missing a stew that was very magical and fell into grace not then it is in man because how to forget or that is how to forget aneth dotas aneth dotas of antonio many and when you know it but one of the most famous aneth dotas of antonio is that he wiped his face with the towel with the that yizú who cleans himself goes to the ass, that is, this type of Aneth Dotas alone cannot screw someone like Antonio, no, he said it himself, that is, he was washing his face and he took a towel to wipe the water off his face and said holy shit holy shit and it was the towel that Yizú uses to clean himself up, he went to his ass tremendously, he didn't have them lying on their faces for half a day, no, I'm not saying this in a bad way, they're Aneth Dotas, what the fuck do they do in the community? It wasn't funny then, I don't know. They make me a little nostalgic for Chavarés because well, that was a good boy, it was the golden age of LOL not many years ago, two thousand seventeen, two thousand eighteen thousand nineteen, the golden age where we had this type of more innocent characters did not end for me, there was not so much evil so much desire to sell you trash to sell teumo etc it was a better time not chavades it was a better time and from here I send my regards to Antonio although later he will give him some stick but but the truth is that it has made me feel sorry I don't It was a shame, then of course Antonio also made a legacy, he didn't make the legacy of Yayans de Guina, Max made a legacy, he started there and ended there, his career was loved, he wasn't loved, I suppose so, to what extent did Yayans use it, could it be the image of the club? No, because maybe Antonio himself generated more attraction towards clients and the big wells than Yayans himself, so I don't know to what extent he took advantage of those Yayans to have it, let's say it was like the same as fake for you, guan The image for Yayans was Antonio's, that is very clear and he made a legacy there and well, it has happened, more than anything without pain or glory, he has managed to become his own extreme and I think he will live off of that, now he will be a content creator. and he will live better, no, but then we will hit him with some stick because now I am talking about him as a person and as a man, well, there is a certain affection for him, not for Antonio, then he has also been like a companion, no, even though we do not know him, no, because he has gone so far. We have seen him evolve from a child to a man, I mean, look at this photo and look at the current photos, I mean, he looks like a guy who has gotten into theft, he looks like a guy who looks like someone else, he doesn't look like someone else, I mean, a little kid, maybe a little dumb guy now It is clear that he still needs some cooking, but he has an image that is more mature, more aggressive, more older, he has not grown and those of us who follow the hero have seen that evolution, no, and that is why I tell you that it makes me a little sad, no, he retires but well currently the image they have is a little bit more I don't know either I see him as a bit more of a bully there is no with his little tattoos and etcetera I don't know if he has the mother's love tattooed somewhere but I suppose that Yes, but it's a bummer, especially because of the fact that the golden age of non-kids is coming to an end. We're seeing the fall of LOL. We're seeing the decline of LOL. And people like Antonio are going to leave. Everyone is going to leave. and the four facts remain and this goes back to being no man's land no and it's a shame no then as a person I already tell you cantoño because he is loved no and he is a guy like this who was a little innocent so and so I in Grace and well he is loved now if we talk about him as a lol player for me just as I have said good things for me as a player today he represents all the mediocrity that exists today in the lec in the European ol represents total mediocrity He no longer told Tila not that he had never worked serious to be anything in him and as a player for me represents that, the lack of ambition, the conformism of saying today I am here I am here is the image of Ya and in his day number 1 I am satisfied, not a person who does not work hard for his dreams, ok, he does not work hard, he prioritizes other things, which, mind you, we all have, in fact, we do the same. I am a soccer player, I have a lot of talent and I do not feel like training hard, and with talent I will be fine playing. I don't know about Albacete, but I could see him playing for Barcelona. He will rise, he has talent, but I hadn't worked hard. I think Cantoño could have been exploited better if they had surrounded him with good staff, a good coach, etc. Antonio could have gotten a little bit better, but he's the living daylights. image or that is to say that you will forgive me for saying this from yesteryear on the day he retires I have already told you good things but as a player for me he is the living image of the mediocrity of conformism what I say about not fighting for your dreams or that is to say it represents Everything that is the sector or that is, it has nothing more than absolutely represents all the parameters, not all the characteristics of the people who are in the lol today, where for them the lol is good, a place where you can have fun from time to time, but not at all. A job, much less a profession, would not be one to which you dedicate hours etc. playing only a little more, then very half-believed, then he also fulfilled the dream of playing in the league, which has been like the cake agenda of his career, which is a must. Of course I didn't have the level to play especially online, so if I say otherwise I can eat my balls, I didn't have the level to play online, always supergapped like 60 minutes behind at minute 15, so some huge gaps, isn't that true? Then in tinfights he did half well, that is to say for tinfights and what varies, he needed a lot of work online, he needed to put in hours, take it seriously, study the game, the interactions, well, what is missing today in the league, that is to say, absolute mediocrity. So he fulfilled that dream of playing in the league that I don't know if they granted it to him because it was a project of the old folks that was not a piece of shit and I said no, well, I'm a cat, I stick to you here and explain what you want and before you retire No, it was already known that he was retiring at that time, those are speculations, so as a player, bad, no, kid, it's bad, as a professional, bad, honestly, he's always more worried about the girl he spent two years with, making a fool of himself. Eat a paella and eat a steak, well, always. She likes it more than that, so it's hard, it's hard, so nothing, kid, I don't want to say much more, the truth is that I'm telling you that it makes me sad, not because of Antonio, but because of seeing that an era is ending, an era is not ending. in the goal a golden era and I already told you that as a person Antonio seems very good to me, I have nothing against him at all, in fact I think I would even have a good laugh with him if I met him but as a professional player I have him that to put the stick because it represents the most absolute mediocrity and the lack of desire and sacrifice no then nothing guys milk on this night of mourning we are going to do a week of mourning on the channel and nothing a greeting to peluquillas from here peluquillas a greeting Commander and we'll see you in the next video guys, a hug to all and a hug Antonio, subscribe and joputas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir las stop words a lista\n",
        "spanish_stop_words = list(nlp.Defaults.stop_words)\n",
        "\n",
        "# Reemplazar en el análisis de temas\n",
        "def analyze_topics_spanish(text, num_topics=5):\n",
        "    vectorizer = CountVectorizer(stop_words=spanish_stop_words)\n",
        "    doc_term_matrix = vectorizer.fit_transform([text])\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
        "    lda.fit(doc_term_matrix)\n",
        "\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for idx, topic in enumerate(lda.components_):\n",
        "        top_terms = [terms[i] for i in topic.argsort()[-5:]]\n",
        "        topics.append(f\"Tema {idx+1}: {', '.join(top_terms)}\")\n",
        "    return topics"
      ],
      "metadata": {
        "id": "VfWURiAEVZjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_topics_spanish(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOUzMqGF5hB8",
        "outputId": "d14374be-b597-4c2d-f07f-678f6d8d852f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tema 1: furbolista, gachiesa, gapéada, genuino, jubilar',\n",
              " 'Tema 2: furbolista, gachiesa, gapéada, genuino, jubilar',\n",
              " 'Tema 3: furbolista, gachiesa, gapéada, genuino, jubilar',\n",
              " 'Tema 4: furbolista, gachiesa, gapéada, genuino, jubilar',\n",
              " 'Tema 5: persona, jugar, acabar, chaval, antonio']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L4em_gac5lwm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}